{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Document Clustering [Dataset: newsgroups5.zip]\n",
    "For this problem you will use a different subset of the 20 Newsgroup data set that you used in Assignment 2  (see the description of the full dataset). The subset for this assignment includes 2,500 documents (newsgroup posts), each belonging to one of 5 categories windows (0), crypt (1), christian (2), hockey (3), forsale (4). The documents are represented by 9328 terms (stems). The dictionary (vocabulary) for the data set is given in the file \"terms.txt\" and the full term-by-document matrix is given in \"matrix.txt\" (comma separated values). The actual category labels for the documents are provided in the file \"classes.txt\". Your goal in this assignment is to perform clustering on the documents and compare the clusters to the actual categories.\n",
    "\n",
    "Your tasks in this problem are the following [Note: for the clustering part of this assignment you should use the kMeans module form Ch. 10 of MLA (use the version provided here as it includes some corrections to the book version). You may also use Pandas and other modules from scikit-learn that you may need for preprocessing or evaluation.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a\n",
    "\n",
    "Create your own distance function that, instead of using Euclidean distance, uses Cosine similarity. This is the distance function you will use to pass to the kMeans function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(v1,v2):\n",
    "    v1_norm = np.linalg.norm(v1)\n",
    "    v2_norm = np.linalg.norm(v2)\n",
    "    cos_sim = (np.dot(v1,v2))/(v1_norm * v2_norm)\n",
    "    distance = 1 - cos_sim\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n",
    "Load the data set [Note: the data matrix provided has terms as rows and documents as columns. Since you will be clustering documents, you'll need to take the transpose of this matrix so that your main data matrix is a document x term matrix. In Numpy, you may use the \".T\" operation to obtain the transpose.] Then, split the data set (the document x term matrix) and set aside 20% for later use (see below). Use the 80% segment for clustering in the next part. The 20% portion must be a random subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'C:\\\\Users\\\\Taha'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('E:/OneDrive/MSPA/CSC 478/Assignment/Assignment 3/newsgroups5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9328, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aargh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaronc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0      aa\n",
       "1   aargh\n",
       "2   aaron\n",
       "3  aaronc\n",
       "4      ab"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_terms = pd.read_csv('terms.txt', na_values = ['?'], dtype = str, header = None)\n",
    "print df_terms.shape\n",
    "df_terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9328, 2500)\n",
      "(2500, 9328)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9318</th>\n",
       "      <th>9319</th>\n",
       "      <th>9320</th>\n",
       "      <th>9321</th>\n",
       "      <th>9322</th>\n",
       "      <th>9323</th>\n",
       "      <th>9324</th>\n",
       "      <th>9325</th>\n",
       "      <th>9326</th>\n",
       "      <th>9327</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 9328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   9318  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "1     0     0     0     0     0     0     0     0     0     1  ...      0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "\n",
       "   9319  9320  9321  9322  9323  9324  9325  9326  9327  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 9328 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matrix = pd.read_csv('matrix.txt', na_values = ['?'], dtype = int, header = None)\n",
    "print df_matrix.shape\n",
    "df_matrix.head()\n",
    "DT_mat = df_matrix.transpose()\n",
    "print DT_mat.shape\n",
    "DT_mat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1\n",
       "0   \n",
       "0  0\n",
       "1  1\n",
       "2  1\n",
       "3  1\n",
       "4  2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_class = pd.read_csv('classes.txt', skiprows= [0], na_values = ['?'], index_col= 0, delim_whitespace= True, header =  None)\n",
    "print df_class.shape\n",
    "df_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_data, test_data, train_label, test_label = train_test_split(DT_mat, df_class, test_size = 0.2, random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Train Data is  (2000, 9328)\n",
      "The shape of Train Label is  (2000, 1)\n",
      "The shape of Test Data is  (500, 9328)\n",
      "The shape of Test Label is  (500, 1)\n"
     ]
    }
   ],
   "source": [
    "print 'The shape of Train Data is ', train_data.shape\n",
    "print 'The shape of Train Label is ', train_label.shape\n",
    "print 'The shape of Test Data is ', test_data.shape\n",
    "print 'The shape of Test Label is ', test_label.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c\n",
    "\n",
    "As in the case of Assignment 2, transform the term-frequencies to tfxidf values. Be sure to maintain DF values for each of the terms in the dictionary. [Note: if you run into problems due to limited computational resources, you may prune the data by removing all terms with low DF values, e.g., terms that appear in less than 10 documents. Be sure to maintain the correspondence between the dictionary terms and the matrix rows.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Train Array is :  (2000L, 9328L)\n",
      "The Shape of Test Array is :  (500L, 9328L)\n"
     ]
    }
   ],
   "source": [
    "train_arr = np.array(train_data)\n",
    "test_arr = np.array(test_data)\n",
    "print \"The shape of Train Array is : \", train_arr.shape\n",
    "print \"The Shape of Test Array is : \", test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of Docs in Training Data is :  2000\n",
      "The Number of Terms in Training Data is :  9328\n",
      "The Number of Docs in Testing Data is :  500\n"
     ]
    }
   ],
   "source": [
    "num_Doc_train = len(train_arr)\n",
    "print \"The Number of Docs in Training Data is : \", num_Doc_train\n",
    "\n",
    "num_Terms_train  = len(train_arr[1])\n",
    "print \"The Number of Terms in Training Data is : \", num_Terms_train\n",
    "\n",
    "num_Doc_test = len(test_arr)\n",
    "print \"The Number of Docs in Testing Data is : \", num_Doc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Documents Frequency Matrix in Training Data is [[ 8  4 17 ...,  3  2  2]]\n",
      "The Term Freq Matrix in Training Data is [14  4 73 ..., 11  5  6]\n"
     ]
    }
   ],
   "source": [
    "Doc_Freq = np.array([(train_arr != 0).sum(0)])\n",
    "print 'The Documents Frequency Matrix in Training Data is', Doc_Freq\n",
    "\n",
    "Term_Freq = train_arr.sum(axis = 0)\n",
    "print 'The Term Freq Matrix in Training Data is', Term_Freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of Training Matrix is  (2000L, 9328L)\n",
      "The Shape of Testing Matrix is  (500L, 9328L)\n"
     ]
    }
   ],
   "source": [
    "train_matrix = np.ones(np.shape(train_arr), dtype =float) *num_Doc_train\n",
    "print 'The Shape of Training Matrix is ', train_matrix.shape\n",
    "test_matrix = np.ones(np.shape(test_arr), dtype = float) * num_Doc_train\n",
    "print 'The Shape of Testing Matrix is ', test_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IDF for Train is: \n",
      "[[ 7.96578428  8.96578428  6.87832144 ...,  9.38082178  9.96578428\n",
      "   9.96578428]\n",
      " [ 7.96578428  8.96578428  6.87832144 ...,  9.38082178  9.96578428\n",
      "   9.96578428]\n",
      " [ 7.96578428  8.96578428  6.87832144 ...,  9.38082178  9.96578428\n",
      "   9.96578428]\n",
      " [ 7.96578428  8.96578428  6.87832144 ...,  9.38082178  9.96578428\n",
      "   9.96578428]\n",
      " [ 7.96578428  8.96578428  6.87832144 ...,  9.38082178  9.96578428\n",
      "   9.96578428]]\n",
      "The IDF for Test is: \n",
      "[[ 7.96578428  8.96578428  6.87832144 ...,  9.38082178  9.96578428\n",
      "   9.96578428]\n",
      " [ 7.96578428  8.96578428  6.87832144 ...,  9.38082178  9.96578428\n",
      "   9.96578428]\n",
      " [ 7.96578428  8.96578428  6.87832144 ...,  9.38082178  9.96578428\n",
      "   9.96578428]\n",
      " [ 7.96578428  8.96578428  6.87832144 ...,  9.38082178  9.96578428\n",
      "   9.96578428]\n",
      " [ 7.96578428  8.96578428  6.87832144 ...,  9.38082178  9.96578428\n",
      "   9.96578428]]\n"
     ]
    }
   ],
   "source": [
    "train_IDF = np.log2(np.divide(train_matrix, Doc_Freq))\n",
    "test_IDF = np.log2(np.divide(test_matrix, Doc_Freq))\n",
    "\n",
    "print 'The IDF for Train is: \\n', train_IDF[0:5]\n",
    "print 'The IDF for Test is: \\n', test_IDF[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of Training Data's TFIDF is:  (2000L, 9328L)\n",
      "The Shape of Testing Data's TFIDF is:  (500L, 9328L)\n"
     ]
    }
   ],
   "source": [
    "TDIDF_train = train_arr * train_IDF\n",
    "print \"The Shape of Training Data's TFIDF is: \", TDIDF_train.shape\n",
    "\n",
    "TDIDF_test = test_arr * test_IDF\n",
    "print \"The Shape of Testing Data's TFIDF is: \", TDIDF_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TDIDF_test = np.nan_to_num(TDIDF_test)\n",
    "TDIDF_train = np.nan_to_num(TDIDF_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d\n",
    "Perform Kmeans clustering on the training data. Write a function to display the top N terms in each cluster along with the cluster DF values for each term, percentage of docs in the cluster in which the terms appear, and the size of the cluster. Sort the terms in decreasing order of the DF percentage. [Extra Credit: use your favorite third party tool, ideally with a Python based API, to create a word cloud for each cluster based on the in-cluster DF values.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randCent(dataSet, k):\n",
    "    n = shape(dataSet)[1]\n",
    "    centroids = zeros((k,n), dtype=float)\n",
    "    for j in range(n): #create random cluster centers\n",
    "        minJ = min(dataSet[:,j])\n",
    "        rangeJ = float(max(dataSet[:,j]) - minJ)\n",
    "        centroids[:,j] = minJ + rangeJ * random.rand(k)\n",
    "    return centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "def kMeans(dataSet, k, distMeas=cosine_sim, createCent=randCent):\n",
    "    m = shape(dataSet)[0]\n",
    "    clusterAssment = zeros((m,2))#create mat to assign data points \n",
    "                                      #to a centroid, also holds SE of each point\n",
    "    centroids = createCent(dataSet, k)\n",
    "    clusterChanged = True\n",
    "    while clusterChanged:\n",
    "        clusterChanged = False\n",
    "        for i in range(m):#for each data point assign it to the closest centroid\n",
    "            minDist = inf; minIndex = -1\n",
    "            for j in range(k):\n",
    "                distJI = distMeas(centroids[j,:],dataSet[i,:])\n",
    "                if distJI < minDist:\n",
    "                    minDist = distJI; minIndex = j\n",
    "            if clusterAssment[i,0] != minIndex: clusterChanged = True\n",
    "            clusterAssment[i,:] = minIndex,minDist**2\n",
    "        # print centroids\n",
    "        for cent in range(k):#recalculate centroids\n",
    "            ptsInClust = dataSet[nonzero(clusterAssment[:,0]==cent)[0]] #get all the point in this cluster - \n",
    "            #Note: this was incorrect in the original distribution.\n",
    "            if(len(ptsInClust)!=0):\n",
    "                centroids[cent,:] = mean(ptsInClust, axis=0) \n",
    "                #assign centroid to mean - Note condition was added 10/28/2013\n",
    "    return centroids, clusterAssment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "center, cluster = kMeans(TDIDF_train,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_TDIDF_train = pd.DataFrame(TDIDF_train)\n",
    "df_TDIDF_test = pd.DataFrame(TDIDF_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def terms_clust(train_data,terms_data, k,N ):\n",
    "    train_mat = np.array(train_data)\n",
    "    center, cluster = kMeans(train_mat, k,distMeas=cosine_sim)\n",
    "    df_clust = pd.DataFrame(cluster,columns = ['Cluster', 'Distance'])\n",
    "    df_cent = pd.DataFrame(center)\n",
    "    \n",
    "    for i in range(k):\n",
    "        c = df_clust[df_clust['Cluster'] == i]\n",
    "        print 'The Size of Cluster ', i, 'is ', len(c)\n",
    "        doc_cluster = train_data.ix[c.index]\n",
    "        doc_array =  np.array(doc_cluster)\n",
    "        DF = np.array([(doc_array.T!=0).sum(1)]).T\n",
    "        tot_doc = len(doc_cluster)\n",
    "        df_DF = pd.DataFrame(DF, columns = ['Count'])\n",
    "        terms_arr = np.array(terms_data)\n",
    "        df_DF['Terms'] = terms_arr\n",
    "        sorted_DF = df_DF.sort_values('Count', ascending= False)\n",
    "        Ser = pd.Series(sorted_DF.Count)\n",
    "        Ser = Ser[:N]\n",
    "        terms =df_terms.ix[Ser.index]\n",
    "        for x in range(len(Ser)):\n",
    "            \n",
    "            print ' The Term \"', terms.iloc[x][0], '\" appears a total of \"', Ser.iloc[x],'\" times and the Percentage in Document Frequency is \"', Ser.iloc[x]/tot_doc, '\"'      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Size of Cluster  0 is  378\n",
      " The Term \" sale \" appears a total of \" 235 \" times and the Percentage in Document Frequency is \" 0.621693121693 \"\n",
      " The Term \" email \" appears a total of \" 144 \" times and the Percentage in Document Frequency is \" 0.380952380952 \"\n",
      " The Term \" pleas \" appears a total of \" 127 \" times and the Percentage in Document Frequency is \" 0.335978835979 \"\n",
      " The Term \" offer \" appears a total of \" 109 \" times and the Percentage in Document Frequency is \" 0.28835978836 \"\n",
      " The Term \" ship \" appears a total of \" 95 \" times and the Percentage in Document Frequency is \" 0.251322751323 \"\n",
      " The Term \" sell \" appears a total of \" 94 \" times and the Percentage in Document Frequency is \" 0.248677248677 \"\n",
      " The Term \" includ \" appears a total of \" 89 \" times and the Percentage in Document Frequency is \" 0.23544973545 \"\n",
      " The Term \" interest \" appears a total of \" 87 \" times and the Percentage in Document Frequency is \" 0.230158730159 \"\n",
      " The Term \" on \" appears a total of \" 84 \" times and the Percentage in Document Frequency is \" 0.222222222222 \"\n",
      " The Term \" thank \" appears a total of \" 82 \" times and the Percentage in Document Frequency is \" 0.216931216931 \"\n",
      "The Size of Cluster  1 is  776\n",
      " The Term \" write \" appears a total of \" 504 \" times and the Percentage in Document Frequency is \" 0.649484536082 \"\n",
      " The Term \" on \" appears a total of \" 399 \" times and the Percentage in Document Frequency is \" 0.514175257732 \"\n",
      " The Term \" articl \" appears a total of \" 379 \" times and the Percentage in Document Frequency is \" 0.488402061856 \"\n",
      " The Term \" know \" appears a total of \" 294 \" times and the Percentage in Document Frequency is \" 0.378865979381 \"\n",
      " The Term \" peopl \" appears a total of \" 293 \" times and the Percentage in Document Frequency is \" 0.377577319588 \"\n",
      " The Term \" think \" appears a total of \" 271 \" times and the Percentage in Document Frequency is \" 0.349226804124 \"\n",
      " The Term \" just \" appears a total of \" 270 \" times and the Percentage in Document Frequency is \" 0.34793814433 \"\n",
      " The Term \" get \" appears a total of \" 237 \" times and the Percentage in Document Frequency is \" 0.305412371134 \"\n",
      " The Term \" god \" appears a total of \" 229 \" times and the Percentage in Document Frequency is \" 0.295103092784 \"\n",
      " The Term \" time \" appears a total of \" 228 \" times and the Percentage in Document Frequency is \" 0.29381443299 \"\n",
      "The Size of Cluster  2 is  443\n",
      " The Term \" window \" appears a total of \" 282 \" times and the Percentage in Document Frequency is \" 0.636568848758 \"\n",
      " The Term \" write \" appears a total of \" 189 \" times and the Percentage in Document Frequency is \" 0.426636568849 \"\n",
      " The Term \" get \" appears a total of \" 147 \" times and the Percentage in Document Frequency is \" 0.331828442438 \"\n",
      " The Term \" on \" appears a total of \" 147 \" times and the Percentage in Document Frequency is \" 0.331828442438 \"\n",
      " The Term \" file \" appears a total of \" 145 \" times and the Percentage in Document Frequency is \" 0.327313769752 \"\n",
      " The Term \" do \" appears a total of \" 136 \" times and the Percentage in Document Frequency is \" 0.306997742664 \"\n",
      " The Term \" thank \" appears a total of \" 135 \" times and the Percentage in Document Frequency is \" 0.304740406321 \"\n",
      " The Term \" know \" appears a total of \" 128 \" times and the Percentage in Document Frequency is \" 0.288939051919 \"\n",
      " The Term \" articl \" appears a total of \" 123 \" times and the Percentage in Document Frequency is \" 0.277652370203 \"\n",
      " The Term \" run \" appears a total of \" 122 \" times and the Percentage in Document Frequency is \" 0.27539503386 \"\n",
      "The Size of Cluster  3 is  398\n",
      " The Term \" write \" appears a total of \" 215 \" times and the Percentage in Document Frequency is \" 0.540201005025 \"\n",
      " The Term \" game \" appears a total of \" 209 \" times and the Percentage in Document Frequency is \" 0.525125628141 \"\n",
      " The Term \" team \" appears a total of \" 188 \" times and the Percentage in Document Frequency is \" 0.472361809045 \"\n",
      " The Term \" articl \" appears a total of \" 168 \" times and the Percentage in Document Frequency is \" 0.422110552764 \"\n",
      " The Term \" plai \" appears a total of \" 162 \" times and the Percentage in Document Frequency is \" 0.407035175879 \"\n",
      " The Term \" go \" appears a total of \" 158 \" times and the Percentage in Document Frequency is \" 0.396984924623 \"\n",
      " The Term \" hockei \" appears a total of \" 157 \" times and the Percentage in Document Frequency is \" 0.394472361809 \"\n",
      " The Term \" on \" appears a total of \" 153 \" times and the Percentage in Document Frequency is \" 0.384422110553 \"\n",
      " The Term \" get \" appears a total of \" 147 \" times and the Percentage in Document Frequency is \" 0.369346733668 \"\n",
      " The Term \" year \" appears a total of \" 126 \" times and the Percentage in Document Frequency is \" 0.316582914573 \"\n",
      "The Size of Cluster  4 is  5\n",
      " The Term \" max \" appears a total of \" 5 \" times and the Percentage in Document Frequency is \" 1.0 \"\n",
      " The Term \" mwt \" appears a total of \" 5 \" times and the Percentage in Document Frequency is \" 1.0 \"\n",
      " The Term \" giz \" appears a total of \" 5 \" times and the Percentage in Document Frequency is \" 1.0 \"\n",
      " The Term \" part \" appears a total of \" 5 \" times and the Percentage in Document Frequency is \" 1.0 \"\n",
      " The Term \" ei \" appears a total of \" 5 \" times and the Percentage in Document Frequency is \" 1.0 \"\n",
      " The Term \" gizw \" appears a total of \" 5 \" times and the Percentage in Document Frequency is \" 1.0 \"\n",
      " The Term \" gizwt \" appears a total of \" 5 \" times and the Percentage in Document Frequency is \" 1.0 \"\n",
      " The Term \" mw \" appears a total of \" 5 \" times and the Percentage in Document Frequency is \" 1.0 \"\n",
      " The Term \" gkw \" appears a total of \" 5 \" times and the Percentage in Document Frequency is \" 1.0 \"\n",
      " The Term \" cliff \" appears a total of \" 5 \" times and the Percentage in Document Frequency is \" 1.0 \"\n"
     ]
    }
   ],
   "source": [
    "terms_clust(df_TDIDF_train,df_terms, 5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e\n",
    "\n",
    "Using the cluster assignments from Kmeans clustering, compare your 5 clusters to the 5 pre-assigned classes by computing the Completeness and Homogeneity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import completeness_score, homogeneity_score\n",
    "\n",
    "center, cluster = kMeans(TDIDF_train,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Completeness Score is 0.837786747868\n",
      "The Homogeneity Score is 0.703868420359\n"
     ]
    }
   ],
   "source": [
    "train_lab_arr = np.array(train_label)\n",
    "new_df = pd.DataFrame(train_lab_arr, columns = ['Classes_Label'])\n",
    "clust_df = pd.DataFrame(cluster,index=df_TDIDF_train.index,copy=True, columns = ['Cluster', 'Distance'])\n",
    "clust_df['Classes_Label'] = train_lab_arr\n",
    "pred_label = np.array(clust_df.Cluster)\n",
    "orig_label = np.array(clust_df.Classes_Label)\n",
    "\n",
    "print 'The Completeness Score is',completeness_score(orig_label, pred_label)\n",
    "print 'The Homogeneity Score is',homogeneity_score(orig_label, pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f \n",
    "\n",
    "Finally, using your cluster assignments as class labels, categorize each of the documents in the 20% set-aside data into each of the appropriate cluster. Your categorization should be based on Cosine similarity between each test document and each cluster centroids. Present your results in a separate file containing the obtained cluster label for each test document as well as Cosine similarities between each test document and each of the 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 9328)\n",
      "(5L, 9328L)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print df_TDIDF_test.shape\n",
    "print center.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_clusters = {}\n",
    "rated = {}\n",
    "pred_ratings = []\n",
    "\n",
    "for i,array in enumerate(TDIDF_test):\n",
    "    \n",
    "    test_clusters[i] = []\n",
    "    \n",
    "    for i2, c in enumerate(center):\n",
    "        \n",
    "        cdistance =  1 - (np.dot(array,c) / (linalg.norm(array) * linalg.norm(c)) )\n",
    "        test_clusters[i].append([i2,cdistance])\n",
    "\n",
    "\n",
    "for key,value in test_clusters.items():\n",
    "\n",
    "        value = sorted(value,key=lambda x:(-x[1],x[0]))\n",
    "        pred = value[1][0]\n",
    "        rated[key] = pred \n",
    "        pred_ratings.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "print len(pred_ratings)\n",
    "print len(test_label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_label_array = np.array(test_label[test_label.columns[0]])\n",
    "newdf = pd.DataFrame()\n",
    "newdf['Original Class'] = test_label_array\n",
    "newdf['Predicted'] = pred_ratings\n",
    "\n",
    "newdf.loc[newdf['Original Class'] == 0 ] = 'Windows'\n",
    "newdf.loc[newdf['Original Class'] == 1 ] = 'Crypt'\n",
    "newdf.loc[newdf['Original Class'] == 2 ] = 'Christian'\n",
    "newdf.loc[newdf['Original Class'] == 3 ] = 'hockey'\n",
    "newdf.loc[newdf['Original Class'] == 4 ] = 'forsale'\n",
    "\n",
    "newdf.loc[newdf['Predicted'] == 0 ] = 'forsale - Predicted'\n",
    "newdf.loc[newdf['Predicted'] == 1 ] = 'Christian - Predicted'\n",
    "newdf.loc[newdf['Predicted'] == 2 ] = 'Windows - Predicted'\n",
    "newdf.loc[newdf['Predicted'] == 3 ] = 'hockey - Predicted'\n",
    "newdf.loc[newdf['Predicted'] == 4 ] = 'Crypt - Predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newdf['Distance'] = test_clusters.items()\n",
    "newdf.to_csv('Result_Test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
